# -*- coding: utf-8 -*-
"""Copy of WindPowerProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/129FoL2Y25bHru8oybX7FF59qHbXqSmF-

# **Wind Power Project **

Import packages
"""

!pip install dtreeviz

import pandas as pd
import numpy as np
from easydict import EasyDict as edict
# https://www.kaggle.com/datasets/theforcecoder/wind-power-forecasting
import matplotlib.pyplot as plt
import fastai
from dtreeviz.trees import *
#from treeinterpreter import treeinterpreter
#from waterfall_chart import plot as waterfall

# from sklearn.datasets import load_iris

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error
from sklearn.impute import SimpleImputer


from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.multioutput import MultiOutputRegressor
from IPython.display import Image, display_svg, SVG

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
#%matplotlib notebook
#%matplotlib ipympl

np.random.seed(2022)
plt.ion()

url = 'https://drive.google.com/uc?id=15LgFZp0GZbobuIfpuWvTm8_VB3ML9Vnq'
wind_data = pd.read_csv(url,low_memory=False)
display(wind_data.head())
target, features = wind_data.columns[1], wind_data.columns[2:]

# which columns will be used for predictions, indepent varaibles that are given
X = wind_data[features]
y = wind_data[target]

X['WindSpeed3'] = X['WindSpeed']**3

I = y.notnull()
X, y = X[I], y[I] # takes care of the missing values

# get rid of any indices for simplicity
X, y = X.reset_index(drop=True), y.values

# Split dataset into train and test -- also has random number generator
# Did not initialize that will give different data everytime you run it
# determinants
# Sort the data in increasing order by time stamp

# find the numeric and non-numeric features
columns1, columns2 = X.select_dtypes(include=['number', ]).columns.tolist(), \
                     X.select_dtypes(exclude=['number', ]).columns.tolist()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)
X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, shuffle=True)

display(columns1)

if len(columns1):
    my_imputer = SimpleImputer(missing_values=np.nan, strategy='median')
    my_imputer.fit(X_train[columns1])  # training imputer
    X_train[columns1] = my_imputer.transform(X_train[columns1])  # imputing the training data
    X_valid[columns1] = my_imputer.transform(X_valid[columns1])  # imputing the validation data
    X_test[columns1] = my_imputer.transform(X_test[columns1])  # imputing the test data

# use most-frequest imputation strategy for non-numeric features
if len(columns2):
    my_imputer2 = SimpleImputer(strategy='most_frequent')
    my_imputer2.fit(X_train[columns2])  # training imputer
    X_train[columns2] = my_imputer2.transform(X_train[columns2])  # imputing the training data
    X_valid[columns2] = my_imputer2.transform(X_valid[columns2])  # imputing the validation data
    X_test[columns2] = my_imputer2.transform(X_test[columns2])  # imputing the test data

# use only the numeric features
# non-numeric feaures need to be handled via an embedding into numeric features

features = columns1

estimators = {
    'tree': DecisionTreeRegressor(max_depth=8, max_leaf_nodes=64, min_samples_leaf=30),
    'knn': KNeighborsRegressor(n_neighbors=5),
    'rf': RandomForestRegressor(max_depth=2, verbose=2),
    'gbr': GradientBoostingRegressor(max_depth=2, verbose=2),
    'xrf': ExtraTreesRegressor(max_depth=8, verbose=2, max_leaf_nodes=64, min_samples_leaf=30),
    #'mor': MultiOutputRegressor(max_depth=8)
}
est_key = 'xrf'
est = estimators[est_key]

est.fit(X_train[features], y_train)

# fit the regressor to the training data
y_pred = est.predict(X_valid[features])  # make predictions on the validation data

fig = plt.figure(figsize = (12,6))
#plt.figure()
I = np.arange(len(y_valid))
plt.scatter(I, y_valid, s=1, label='true')
plt.scatter(I, y_pred, s=1, label='pred')
plt.ylabel('y')
plt.xlabel('index')
plt.legend()
plt.title(f'{est_key} predictions on validation data')
plt.show()

est_perf = edict(rmse=np.sqrt(mean_squared_error(y_valid, y_pred)),
                 mae=mean_absolute_error(y_valid, y_pred),
                 mdae=median_absolute_error(y_valid, y_pred))

msg = [f'{k}={v:.2f}' for k, v in est_perf.items()]
tab = "\t"
print(f'{est_key:10s} validation {tab.join(msg)}')
# plot of the errors
# x axis -- y_valid
# lowest to highest power
# plt.plot(x=y_valid, y=y_pred-y_valid)
# S = np.argsort(y_valid)

e = y_pred - y_valid
fig, axs = plt.subplots(nrows=1, ncols=2)
axs[0].scatter(np.arange(len(e)), e, s=1)
axs[0].set_title(f'prediction errors')
axs[0].set_ylabel('y_pred - y_true')
axs[0].set_xlabel('index')
axs[1].hist(e, bins=50, density=True)  # to see if this bell curve has the normal distribution
axs[1].set_xlabel('error')
axs[1].set_ylabel('probability')
axs[1].set_title(f'density of prediction errors')
fig.suptitle(f'{est_key} regressor {" ".join(msg)}')

plt.show()
plt.pause(1) # to see if it is not showing
print('done')

type(est),type(X_train), type(y_train),pd.Series(y_train)

x_data=X_train[features][:1000].values
y_data=pd.Series(y_train[:1000]).values

type(features)

type(x_data), type(y_data)

est.fit(x_data, y_data)

# Try to make the tree using dtreeviz
viz = dtreeviz(est, 
               x_data,
               y_data,
               target_name = target, 
               feature_names=features,
               fontname="DejaVu Sans",
               title_fontsize=16,
               colors = {"title":"purple"})
               #target_name=target,
               #feature_names=features)
#viz = dtreeviz(est, X_train.,feature_names=features,target_name=target,
               #fontname='DejaVu Sans', scale=1.6, label_fontsize=10, orientation='LR', X=X_valid)
viz
# will memorize, but not evaluate

"""https://en.wikipedia.org/wiki/Betz%27s_law

"""